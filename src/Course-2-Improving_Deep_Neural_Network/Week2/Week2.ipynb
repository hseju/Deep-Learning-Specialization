{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "009e817a-bc1a-4e1c-9ccb-e3057e9161db",
   "metadata": {},
   "source": [
    "# WEEK 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773625e1-ab70-469a-b8c5-ad51b169fb07",
   "metadata": {},
   "source": [
    "### Mini Batch Gradient Descent\n",
    "\n",
    "When the number of training examples are very large in millions, it is recommended to use mini batch gradient descent to optimize the cost function and create the model. The idea here is to divide the whole training set into t number of mini batches and each batch will have fixed number of training examples.\n",
    "\n",
    "When the size of the mini batch is 1. which typically means that the all the training examples are optimized at the same time and it is called stochastic gradient descent. \n",
    "\n",
    "When the size of the mini batch is m (number of training examples), it means that each training examples is going through the iterative process to calculate cost function and it becomes a very lengthy process. This is called batch gradient descent. This is not recommended when the training set size is too large. \n",
    "\n",
    "Mini batch falls in between these two where certain number of training examples are grouped together in number of batches and each batch is then calculated for its cost function and gradient descent.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d161afa3-c52b-4975-b539-70c1b3535990",
   "metadata": {},
   "source": [
    "### Momentum Gradient Descent\n",
    "\n",
    "This utilizes the exponentially weighted averages of weights and biases to smooth out the cost function parameter simulatneously increasing the learning speed and reaching the minimum cost. \n",
    "\n",
    "After computing the back propagation wieghts and biases that is $ dW $ and $db$, new values of weights and biases are calculated as below using exponential weighted average equation.\n",
    "\n",
    "$$ V_{dW} = \\beta V_{dW} + (1-\\beta) dW $$\n",
    "\n",
    "$$ W = W - \\alpha V_{dW} $$\n",
    "\n",
    "and biases as,\n",
    "\n",
    "$$ V_{db} = \\beta V_{db} + (1-\\beta) db $$\n",
    "\n",
    "$$ b = b - \\alpha V_{db} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a9b353-75a7-43a8-9041-9e460ce672a8",
   "metadata": {},
   "source": [
    "### RMSprop Optimization algorithm\n",
    "\n",
    "Root mean squared optimization algorithm utilizes the square root values of weighted average of weights and biases to decrease the amount of fluctuation in the gradient descent and increase the speed of optimization. \n",
    "\n",
    "$$ S_{dW}  = \\beta S_{dW} + (1-\\beta) dW^2 $$\n",
    "\n",
    "$$ W = W - \\alpha \\frac{dW} {\\sqrt{S_{dW}} + \\epsilon} $$\n",
    "\n",
    "where $\\epsilon $ is the added parameter which can be set to a extremely low value just to make sure the denominator does not equals to zero. Similarly for bias the equation will be,\n",
    "\n",
    "$$ S_{db}  = \\beta S_{db} + (1-\\beta) db^2 $$\n",
    "\n",
    "$$ b = b - \\alpha \\frac{db} {\\sqrt{S_{db}} + \\epsilon} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d72363-b924-4e0a-aadb-e9e297dff2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
